#!/usr/bin/env python3
"""Lightweight duplication scanner for modular codebases.

Detects:
- exact duplicate files (normalized whitespace)
- repeated normalized code blocks across C/C++ files
"""
from __future__ import annotations

from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
import hashlib
import re

ROOT = Path(__file__).resolve().parents[1]
CODE_EXTS = {".cpp", ".h", ".hpp", ".cc", ".c"}
IGNORE_PREFIXES = {".git/", "build/", "vendor/", "lib/", "logs/"}


def ignored(rel: str) -> bool:
    return any(rel.startswith(p) for p in IGNORE_PREFIXES)


def normalize_text(text: str) -> str:
    text = re.sub(r"//.*", "", text)
    text = re.sub(r"/\*.*?\*/", "", text, flags=re.S)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def iter_code_files():
    for p in ROOT.rglob("*"):
        if not p.is_file() or p.suffix not in CODE_EXTS:
            continue
        rel = p.relative_to(ROOT).as_posix()
        if ignored(rel):
            continue
        yield p, rel


def scan_exact_duplicates():
    buckets: dict[str, list[str]] = defaultdict(list)
    for p, rel in iter_code_files():
        norm = normalize_text(p.read_text(errors="ignore"))
        if not norm:
            continue
        digest = hashlib.sha256(norm.encode()).hexdigest()
        buckets[digest].append(rel)
    return [v for v in buckets.values() if len(v) > 1]


@dataclass
class BlockHit:
    file: str
    start_line: int


def scan_block_duplicates(block_lines: int = 10):
    blocks: dict[str, list[BlockHit]] = defaultdict(list)
    for p, rel in iter_code_files():
        lines = p.read_text(errors="ignore").splitlines()
        cleaned = [normalize_text(ln) for ln in lines]
        cleaned = [ln for ln in cleaned if ln]
        for i in range(0, max(0, len(cleaned) - block_lines + 1)):
            block = "\n".join(cleaned[i:i + block_lines])
            h = hashlib.md5(block.encode()).hexdigest()
            blocks[h].append(BlockHit(rel, i + 1))

    clusters = []
    for hits in blocks.values():
        files = {h.file for h in hits}
        if len(files) > 1:
            clusters.append(hits)
    clusters.sort(key=lambda x: len({h.file for h in x}), reverse=True)
    return clusters[:30]


def write_report(exact_dups, block_dups):
    out = ROOT / "docs" / "duplication_report.md"
    lines = ["# Duplication Report", "", "Generated by `tools/scan_code_duplication.py`.", ""]

    lines += ["## Exact duplicate files", ""]
    if not exact_dups:
        lines += ["- None detected."]
    else:
        for grp in exact_dups:
            lines += [f"- Group ({len(grp)} files):"]
            for f in grp:
                lines += [f"  - `{f}`"]

    lines += ["", "## Repeated code blocks (cross-file)", ""]
    if not block_dups:
        lines += ["- None detected."]
    else:
        for i, hits in enumerate(block_dups, start=1):
            files = sorted({h.file for h in hits})
            lines += [f"### Cluster {i} ({len(files)} files)"]
            for h in hits[:8]:
                lines += [f"- `{h.file}` @ line {h.start_line}"]
            if len(hits) > 8:
                lines += [f"- ... {len(hits)-8} more locations"]
            lines += [""]

    lines += ["## Recommendations", "", "1. Extract repeated helpers into shared utility modules.",
              "2. Add this scan to CI and fail when duplication exceeds threshold.",
              "3. Prioritize high-risk duplicates in security, compliance, and API parsing."]

    out.write_text("\n".join(lines) + "\n")
    print(f"Wrote {out.relative_to(ROOT)}")


def main():
    exact = scan_exact_duplicates()
    blocks = scan_block_duplicates(10)
    write_report(exact, blocks)
    print(f"Exact duplicate groups: {len(exact)}")
    print(f"Block duplication clusters: {len(blocks)}")


if __name__ == "__main__":
    main()
